{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests as r\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from requests.exceptions import ConnectionError\n",
    "import pymongo\n",
    "\n",
    "\n",
    "#這裡是for要寫進MongoDB用的\n",
    "from pymongo import MongoClient\n",
    "#uri = \"mongodb://USERNAME:password@host?authSource=source\" \n",
    "client = MongoClient(\"10.120.28.17\", 27017)\n",
    "\n",
    "#我們自己訂的db 和 collection\n",
    "db = client.tz\n",
    "collect = db.testdb\n",
    "\n",
    "\n",
    "#years = range(2004,2005)\n",
    "\n",
    "def truemovie_crawler(years):\n",
    "    \"\"\" 觸電網爬蟲，完整版\n",
    "          輸入值：year = 你要爬取的年份\n",
    "          輸出值：\"Crawler finished\"\n",
    "          NOTE: 在function中，直接將每一個document送出去給MongoDB了。\n",
    "    \"\"\"\n",
    "    isnext = '類型'.decode(\"utf8\")\n",
    "    isnext1 = '中文片名'.decode(\"utf8\")\n",
    "    isnext2 = '英文片名'.decode(\"utf8\")\n",
    "    ch1 = \"：\".decode(\"utf8\")\n",
    "    ch2 = \"、\".decode(\"utf8\")\n",
    "\n",
    "    # movoe list 用來放所有爬下來的資料\n",
    "    movie_list = []\n",
    "    # 用來放沒有成功爬下來的資料，到時候再爬一次 !!!重新再倒入的function還未寫喔!!!\n",
    "    failed_urls = []\n",
    "    post_number = []\n",
    "    \n",
    "    movie_url_head = 'http://www.truemovie.com/'\n",
    "    movie_url_tail = 'moviedata/'\n",
    "    title_list = []\n",
    "\n",
    "    for year in years: \n",
    "\n",
    "        url_year = movie_url_head + str(year) + movie_url_tail\n",
    "        res = r.get(url_year)\n",
    "        res.encoding = 'big5'\n",
    "        soup = BeautifulSoup(res.text, \"lxml\")\n",
    "        urls = soup.find_all(href=re.compile(\"htm\"))\n",
    "\n",
    "        #print urls\n",
    "\n",
    "        for b in urls:\n",
    "\n",
    "            try:\n",
    "\n",
    "                # 下面這些東西，每部電影都要是新的，所以放在for迴圈裡，每次進來都更新\n",
    "                type_list = []\n",
    "                name = \"\"\n",
    "                e_name = \"\"\n",
    "                dic = {}\n",
    "\n",
    "                if b['href']== 'AD.htm':\n",
    "                    print \"ccc\"\n",
    "                    continue\n",
    "\n",
    "                url = url_year + b['href']\n",
    "                res = r.get(url)\n",
    "                res.encoding = 'big5'\n",
    "                soup = BeautifulSoup(res.text, \"lxml\")\n",
    "\n",
    "                #有些放在font內，有些放在P內故寫兩種\n",
    "\n",
    "                url1 = soup.find_all(\"p\")\n",
    "                url2 = soup.find_all(\"font\")\n",
    "                a = 0\n",
    "                \n",
    "                try:\n",
    "                    for c in url1:\n",
    "                        if re.search(isnext1,c.text):\n",
    "                            for n in c.text.split(ch1)[1]:\n",
    "                                name += n\n",
    "\n",
    "                    for c in url1:\n",
    "                        if re.search(isnext2,c.text):\n",
    "                            for n in c.text.split(ch1)[1]:\n",
    "                                e_name += n\n",
    "\n",
    "                    for c in url1:\n",
    "                        if re.search(isnext,c.text):\n",
    "                            for t in c.text.split(ch1)[1].split(ch2):\n",
    "                                type_list.append(t)\n",
    "                                a = 1\n",
    "\n",
    "                    # 一部電影in一個dict\n",
    "                    dic.update({\"title_t\":name,\"title_e \":e_name,\"type\":type_list,\"url\":url})\n",
    "                    # 先把這些dict存進一個list,之後改寫成json直接傳進MongoDB\n",
    "    #                movie_list.append(dic)\n",
    "\n",
    "                    #存入MongoDB用，明天試\n",
    "                    # 直接將document放進MongoDB的collection中\n",
    "                    post_id = collect.insert_one(dic).inserted_id\n",
    "                    # 每一筆document紀錄id\n",
    "                    post_number.append(post_id)\n",
    "                    # 算存進去的數目\n",
    "                    len(post_number)\n",
    "\n",
    "    #                print dic\n",
    "\n",
    "                    if a==0 :\n",
    "                        for c in url2:\n",
    "                            if re.search(isnext1,c.text):\n",
    "                                for n in c.text.split(ch1)[1]:\n",
    "                                    name += n\n",
    "\n",
    "                        for c in url2:\n",
    "                            if re.search(isnext2,c.text):\n",
    "                                for n in c.text.split(ch1)[1]:\n",
    "                                    e_name += n\n",
    "\n",
    "                        for c in url2:\n",
    "                            if re.search(isnext,cl.text):\n",
    "                                for t in c.text.split(ch1)[1].split(ch2):\n",
    "                                    type_list.append(t)\n",
    "\n",
    "                        dic.update({\"title_t\":name,\"title_e \":e_name,\"type\":type_list,\"url\":url})\n",
    "    #                   movie_list.append(dic)\n",
    "\n",
    "                        #存入MongoDB用，明天試\n",
    "                        # 直接將document放進MongoDB的collection中\n",
    "                        post_id = collect.insert_one(dic).inserted_id\n",
    "                        # 每一筆document紀錄id\n",
    "                        post_number.append(post_id)\n",
    "                        # 算存進去的數目\n",
    "                        #len(post_number)\n",
    "\n",
    "    #                    print dic\n",
    "                    \n",
    "                except pymongo.errors.DuplicateKeyError as e:\n",
    "                    print \"===\"\n",
    "                    continue\n",
    "\n",
    "            except ConnectionError as e:\n",
    "                #print  '111'\n",
    "                failed_urls.append(url_year + b['href'])\n",
    "                print url_year+b['href']\n",
    "    \n",
    "    try_to_death(failed_urls)\n",
    "    return failed_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def try_to_death(retry_url):\n",
    "    \n",
    "    while(len(retry_url)>0):\n",
    "        retry_url = movie_loop(retry_url)        \n",
    "        print len(retry_url)\n",
    "    return \"Finished\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
